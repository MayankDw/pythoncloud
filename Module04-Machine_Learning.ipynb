{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/roitraining/techtrek-python/blob/main/Module04-Machine_Learning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning with Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zLiYf9fYm8qq"
   },
   "source": [
    "### The following is a list of some of the most popular packages used in ML.\n",
    "* Sometimes we might build a machine with different combinations, but for our examples we'll just install them all.\n",
    "* Note these are already pre-installed, so don't run the following. It's just there for reference.\n",
    "  * If you do this on your own in a Colab Notebook or one of your own, you'd have to make sure these packages are installed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XB_8WTPLmtvD"
   },
   "outputs": [],
   "source": [
    "! pip install pandas scikit-learn tensorflow tensorboard matplotlib seaborn theano bokeh keras nltk joblib pyspark torch\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f8gc-TGRmtvC"
   },
   "source": [
    "## Cluster Analysis\n",
    "* Cluster Analysis is an unsupervised model to help identify natural patterns that may exist in data.\n",
    "* It is often a preliminary step used in Exploratory Data Analysis (EDA) to assist ML Practitioners in understanding their data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2SftnAf9kz44"
   },
   "source": [
    "### Let's create a random data set with two features that is clustered around three center points.\n",
    "* Normally we'd read a real data set from a file, but for learning purposes this random set will be clearer.\n",
    "* The <font color='blue' face=\"Courier New\" size=\"+1\">x</font> data represents two features, such as Age and Income, Temperature and Humidity.\n",
    "* The <font color='blue' face=\"Courier New\" size=\"+1\">y</font> data indicates which group or cluster it belongs to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "j4PBOdBbkz5K"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.datasets import make_blobs\n",
    "\n",
    "# Creating a sample dataset with 4 clusters\n",
    "x, y = make_blobs(n_samples=400, n_features=2, centers=3)\n",
    "\n",
    "display(x[:10]) # features\n",
    "display(y[:5]) # cluster member\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WfgLfS9Fkz5l"
   },
   "source": [
    "### The raw numbers are hard to understand, so let's plot it first to visualize \n",
    "* Doing so we can clearly see the data tends to have natural groupings.\n",
    "* With real data and with many more dimensions, this is much harder to see but this is meant to help us understand the concept. From here, we can do our best to imagine it in multiple dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bLOUIX7zkz5u"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['figure.figsize'] = (16, 9)\n",
    "plt.plot(x[:,0],x[:,1],'o')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gh45c8f4kz6C"
   },
   "source": [
    "### With real data we would not have the <font color='blue' face=\"Courier New\" size=\"+1\">y</font> data that indicates which cluster each point belongs to, so we would run a cluster model to help figure that out.\n",
    "* The algorithm does this by comparing each point to every other point and calculating the distances between them to determine which are closest to one another. Sort of like a birds of a feather flock together concept.\n",
    "* To do this, we tell it we want to use a <font color='blue'>cluster algorithm</font> in this case <font color='blue'>KMeans</font>.\n",
    "  * It's one of several different algorithms that could do this kind of analysis using different math.\n",
    "* The <font color='blue' face=\"Courier New\" size=\"+1\">fit</font> method tells it to start doing the necessary calculations. This is often known as the <font color='blue'>training phase</font>.\n",
    "* When all the math is done, it returns which cluster it thinks each point belongs to and what is the center point of each cluster.\n",
    "* We do need to have an idea of how many clusters we think there should be. In this case, it is three. Changing this value will change the outcome and it just takes experimenting to get the right value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "T7JXhyPVkz6J",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn import cluster\n",
    "CLUSTERS = 3\n",
    "k_means = cluster.KMeans(n_clusters=CLUSTERS, random_state = 12)\n",
    "print('labels_' in dir(k_means))\n",
    "k_means.fit(x)\n",
    "print('labels_' in dir(k_means))\n",
    "print(k_means.labels_)\n",
    "print(k_means.cluster_centers_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3thzYHA_kz6Z"
   },
   "source": [
    "### Let's combine this new information about clusters to make the plot clearer.\n",
    "* We'll use the cluster number the model gave us to change the point's color.\n",
    "* We'll use the center points of each cluster the model calculated to mark an X."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Y8VdZnHskz6a"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "def plot_cluster(model, data, clusters):\n",
    "    labels = model.labels_\n",
    "    centroids = model.cluster_centers_\n",
    "\n",
    "    for i in range(clusters):\n",
    "        ds = data[np.where(labels==i)]\n",
    "        # plot the data observations\n",
    "        plt.plot(ds[:,0],ds[:,1],'o')\n",
    "        # plot the centroids\n",
    "        lines = plt.plot(centroids[i,0],centroids[i,1],'kx')\n",
    "    plt.show()\n",
    "\n",
    "plot_cluster(k_means, x, CLUSTERS)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "csjnMPMSkz6h"
   },
   "source": [
    "### Guessing how many clusters is appropriate is tricky, but we can use an <font color='blue'>elbow chart</font> to help figure out how many clusters might be the most natural and combine that with our skills as an analyst and knowledge of the business use case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vaOxe8Lnkz6k"
   },
   "outputs": [],
   "source": [
    "def plot_elbow(data, cluster_cnt = 6):\n",
    "   CLUSTERS = range(1, cluster_cnt + 1)\n",
    "   kmeans = [cluster.KMeans(n_clusters=i) for i in CLUSTERS]\n",
    "\n",
    "   score = [kmeans[i].fit(data).score(data) for i in range(len(kmeans))]\n",
    "   #print(score)\n",
    "   plt.plot(CLUSTERS ,score)\n",
    "   plt.xlabel('Number of Clusters')\n",
    "   plt.ylabel('Score')\n",
    "   plt.title('Elbow Curve')\n",
    "   plt.xticks(np.arange(1, cluster_cnt + 1, 1))\n",
    "   plt.show()\n",
    "\n",
    "plot_elbow(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "umNfK-ddFbQu"
   },
   "source": [
    "* In this case, the elbow indicates that three is probably the right choice.\n",
    "* There are other techniques that can do this in more detail.\n",
    "* Below is an example of a <font color='blue'>silhouette chart</font>.\n",
    "* You can expand this and explore it on your own later.\n",
    "\n",
    "### You can expand this and explore it on your own later. ##\n",
    "#### Just copy this code into the empty code block below and run it.\n",
    "<br>\n",
    "\n",
    "<details><summary>Click for <b>code</b></summary>\n",
    "<p>\n",
    "\n",
    "```python\n",
    "%matplotlib inline\n",
    "\n",
    "def silhouette_plot(data, count = 6):\n",
    "   from sklearn.datasets import make_blobs\n",
    "   from sklearn.cluster import KMeans\n",
    "   from sklearn.metrics import silhouette_samples, silhouette_score\n",
    "\n",
    "   import matplotlib.pyplot as plt\n",
    "   import matplotlib.cm as cm\n",
    "   import numpy as np\n",
    "\n",
    "# Generating the sample data from make_blobs\n",
    "# This particular setting has one distinct cluster and 3 clusters placed close\n",
    "# together.\n",
    "#X, y = make_blobs(n_samples=500, n_features=2, centers=4, cluster_std=1, center_box=(-10.0, 10.0), shuffle=True, random_state=1)  # For reproducibility\n",
    "\n",
    "   range_n_clusters = range(2, count + 1)\n",
    "\n",
    "   for n_clusters in range_n_clusters:\n",
    "       # Create a subplot with 1 row and 2 columns\n",
    "       fig, (ax1, ax2) = plt.subplots(1, 2)\n",
    "       fig.set_size_inches(18, 7)\n",
    "\n",
    "       # The 1st subplot is the silhouette plot\n",
    "       # The silhouette coefficient can range from -1, 1 but in this example all\n",
    "       # lie within [-0.1, 1]\n",
    "       ax1.set_xlim([-0.1, 1])\n",
    "       # The (n_clusters+1)*10 is for inserting blank space between silhouette\n",
    "       # plots of individual clusters, to demarcate them clearly.\n",
    "       ax1.set_ylim([0, len(data) + (n_clusters + 1) * 10])\n",
    "\n",
    "       # Initialize the clusterer with n_clusters value and a random generator\n",
    "       # seed of 10 for reproducibility.\n",
    "       clusterer = KMeans(n_clusters=n_clusters, random_state=10)\n",
    "       cluster_labels = clusterer.fit_predict(data)\n",
    "\n",
    "       # The silhouette_score gives the average value for all the samples.\n",
    "       # This gives a perspective into the density and separation of the formed\n",
    "       # clusters\n",
    "       silhouette_avg = silhouette_score(data, cluster_labels)\n",
    "       print(\"For n_clusters =\", n_clusters, \"The average silhouette_score is :\", silhouette_avg)\n",
    "\n",
    "       # Compute the silhouette scores for each sample\n",
    "       sample_silhouette_values = silhouette_samples(data, cluster_labels)\n",
    "\n",
    "       y_lower = 10\n",
    "       for i in range(n_clusters):\n",
    "           # Aggregate the silhouette scores for samples belonging to\n",
    "           # cluster i, and sort them\n",
    "           ith_cluster_silhouette_values = sample_silhouette_values[cluster_labels == i]\n",
    "\n",
    "           ith_cluster_silhouette_values.sort()\n",
    "\n",
    "           size_cluster_i = ith_cluster_silhouette_values.shape[0]\n",
    "           y_upper = y_lower + size_cluster_i\n",
    "\n",
    "           color = cm.nipy_spectral(float(i) / n_clusters)\n",
    "           ax1.fill_betweenx(np.arange(y_lower, y_upper), 0, ith_cluster_silhouette_values, facecolor=color, edgecolor=color, alpha=0.7)\n",
    "\n",
    "           # Label the silhouette plots with their cluster numbers at the middle\n",
    "           ax1.text(-0.05, y_lower + 0.5 * size_cluster_i, str(i))\n",
    "\n",
    "           # Compute the new y_lower for next plot\n",
    "           y_lower = y_upper + 10  # 10 for the 0 samples\n",
    "\n",
    "       ax1.set_title(\"The silhouette plot for the various clusters.\")\n",
    "       ax1.set_xlabel(\"The silhouette coefficient values\")\n",
    "       ax1.set_ylabel(\"Cluster label\")\n",
    "\n",
    "       # The vertical line for average silhouette score of all the values\n",
    "       ax1.axvline(x=silhouette_avg, color=\"red\", linestyle=\"--\")\n",
    "\n",
    "       ax1.set_yticks([])  # Clear the yaxis labels / ticks\n",
    "       ax1.set_xticks([-0.1, 0, 0.2, 0.4, 0.6, 0.8, 1])\n",
    "\n",
    "       # 2nd Plot showing the actual clusters formed\n",
    "       colors = cm.nipy_spectral(cluster_labels.astype(float) / n_clusters)\n",
    "       ax2.scatter(data[:, 0], data[:, 1], marker='.', s=30, lw=0, alpha=0.7, c=colors, edgecolor='k')\n",
    "\n",
    "       # Labeling the clusters\n",
    "       centers = clusterer.cluster_centers_\n",
    "       # Draw white circles at cluster centers\n",
    "       ax2.scatter(centers[:, 0], centers[:, 1], marker='o', c=\"white\", alpha=1, s=200, edgecolor='k')\n",
    "\n",
    "       for i, c in enumerate(centers):\n",
    "           ax2.scatter(c[0], c[1], marker='$%d$' % i, alpha=1, s=50, edgecolor='k')\n",
    "\n",
    "       ax2.set_title(\"The visualization of the clustered data.\")\n",
    "       ax2.set_xlabel(\"Feature space for the 1st feature\")\n",
    "       ax2.set_ylabel(\"Feature space for the 2nd feature\")\n",
    "\n",
    "       plt.suptitle((\"Silhouette analysis for KMeans clustering on sample data with n_clusters = %d\" % n_clusters), fontsize=14, fontweight='bold')\n",
    "\n",
    "   plt.show()\n",
    "\n",
    "silhouette_plot(x, 6)\n",
    "```\n",
    "</p>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "I-6ZvIuEkz6r"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hGFlX458FbQv"
   },
   "source": [
    "## Regression Analysis\n",
    "* Regression Analysis is a supervised model, meaning we must first train the model and then we can use it to make predictions.\n",
    "* It is used whenever we want to predict a numerical value.\n",
    "* Let's import a CSV with some housing sales data.\n",
    "* We're also applying some formatting to make it easier to understand the numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "b7tzI-jjmtvG"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.options.display.float_format = '${:,.2f}'.format\n",
    "\n",
    "USAhousing = pd.read_csv('USA_Housing.csv')\n",
    "print(USAhousing.columns)\n",
    "C = USAhousing.iloc[:20].style.format({'Avg. Area Income': '${:,.2f}'\n",
    "                             , 'Avg. Area House Age': '{:,.1f}'\n",
    "                             , 'Avg. Area Number of Rooms': '{:,.1f}'\n",
    "                             , 'Avg. Area Number of Bedrooms': '{:,.1f}'\n",
    "                             , 'Area Population': '{:,.0f}'\n",
    "                             , 'Price': '${:,.2f}'})\n",
    "\n",
    "display(C)\n",
    "\n",
    "#display(USAhousing)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FJALrnz2FbQv"
   },
   "source": [
    "* Of all these columns we could come up with, the hypothesis that income, age, number of rooms and bedrooms, and population might affect the price. So we will keep those columns as features.\n",
    "* The whole address is too unique to be a good feature.\n",
    "  * We could parse it to extract state or town and that might influence the price.\n",
    "  * But for simplicity, we will just leave it out.\n",
    "  * There's usually some data we have that we don't include as a feature.\n",
    "* So let's keep the columns we want as features, indicate that price is the target and then split this into two sets.\n",
    "  * The training set is used to do the calculations for the regression.\n",
    "  * The testing set is used to see how good of a job the training did by comparing the actual price to the predicted price."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZweZznGgmtvH"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x = USAhousing[['Avg. Area Income', 'Avg. Area House Age', 'Avg. Area Number of Rooms',\n",
    "               'Avg. Area Number of Bedrooms', 'Area Population']]\n",
    "y = USAhousing['Price']\n",
    "\n",
    "trainX, testX, trainY, testY = train_test_split(x, y, test_size = 0.4, random_state = 101)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G8A8KDutFbQv"
   },
   "source": [
    "* Now we can train it with the feature and target training set.\n",
    "* Then we can use the reserved training set to get values for what that model would predict the price should be if it had to guess."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "udRIgD1tmtvH"
   },
   "outputs": [],
   "source": [
    "from pandas import DataFrame\n",
    "from sklearn.linear_model import LinearRegression\n",
    "lm = LinearRegression()\n",
    "lm.fit(trainX, trainY)\n",
    "predictions = lm.predict(testX)\n",
    "\n",
    "df = DataFrame({'Actual': testY[:10], 'Predicted': predictions[:10]})\n",
    "\n",
    "\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OzSskhkbFbQv"
   },
   "source": [
    "* Just by eyeballing it, we can see it did an OK job overall; some are pretty close and some are way off.\n",
    "* Maybe our model needs more or better data, either more features or more rows.\n",
    "* Sometimes we might try using different algorithms or changing <font color='blue'>hyperparameters</font> that affect how an algorithm might work.\n",
    "* Either way, we first need to quantify how good this model did with some metrics.\n",
    "  * There are many metrics and each has its pros and cons.\n",
    "  * Let's explore the basics here of <font color='blue'>MSE</font> and <font color='blue'>r-squared</font>.\n",
    "* The coefficients indicate how much to multiply each feature value by, then add them up and you get the predicted price.\n",
    "* In this case, the r-squared of 92% means it's not bad, it could maybe be better, but it's certainly better than a random guess."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "A79bjIFhmtvH"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "print(\"Mean squared error: %.2f\" % mean_squared_error(testY, predictions))\n",
    "print('r-squared: %.2f' % r2_score(testY, predictions))\n",
    "print(lm.coef_, lm.intercept_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ANJ-HCJdFbQv"
   },
   "source": [
    "#### You can do a deeper dive into Regression on your own. There are many more algorithms, hyperparmeters, metrics, and packages that can be used and compared to one another on the same data set. Our job is to do as many as we can to find the best overall for our business use case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mhhblPvZFbQv"
   },
   "source": [
    "## Let's explore classification analysis.\n",
    "* This is used to predict an outcome instead of a numeric value.\n",
    "* It could be a yes/no, true/false outcome which we typically call <font color='blue'>binomial</font> classification.\n",
    "* But it could also be predicting one of three or more outcomes which we call <font color='blue'>multiclass</font> classification.\n",
    "    * Is a potential borrower low risk, medium risk, high risk or toxic?\n",
    "    * Does a patient have a risk of Type 1, Type 2, or no diabetes?\n",
    "* Let's load a dataset and try to predict if there is a possible fraudelent transaction or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hXEpz4AXFbQv"
   },
   "outputs": [],
   "source": [
    "! wget \"https://storage.googleapis.com/joey-public-bucket/python_deloitte/CreditCardFraud_1M.csv\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oudKKlbMmtvF"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('CreditCardFraud_1M.csv')\n",
    "display(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h0q1XH3BFbQv"
   },
   "source": [
    "* We will try to predict if it isFraud, so that will be our <font color='blue'>target</font> or <font color='blue'>label</font> when we are doing classification.\n",
    "* Let's keep some columns and ignore some we think won't influence the outcome.\n",
    "* The thing is, features all need to be numeric, so non-numeric values like type are often call <font color='blue'>categorical</font> and can be converted into a numeric representation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "m3H5L3n7mtvF"
   },
   "outputs": [],
   "source": [
    "columns = ['type', 'amount', 'oldbalanceOrg', 'newbalanceOrig', 'oldbalanceDest', 'newbalanceDest', 'isFraud']\n",
    "df = df[columns]\n",
    "df.type = pd.Categorical(df.type).codes\n",
    "print(df.shape, df.columns)\n",
    "display(df.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7OlC6xX1FbQw"
   },
   "source": [
    "* Similar to regression, we will split the set up into a training and testing set.\n",
    "* Because of limitations as to how much we can load into memory on this machine, we will just take a small sample of 30% for training and 10% for testing.\n",
    "* We also want to make sure that the number of fraud and non-fraud records in the two sets are roughly the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_RKjqWjDmtvG"
   },
   "outputs": [],
   "source": [
    "train_size = .3\n",
    "test_size = .1\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import preprocessing as pp\n",
    "dfNB = df\n",
    "trainNB_X, testNB_X, trainNB_Y, testNB_Y = train_test_split(dfNB[dfNB.columns[:-1]], dfNB.isFraud, \\\n",
    "                                        train_size = train_size, test_size = test_size, random_state = 1)\n",
    "\n",
    "print('Train Set Percentages', trainNB_Y.value_counts()/trainNB_Y.count())\n",
    "print('Test Set Percentages', testNB_Y.value_counts()/testNB_Y.count())\n",
    "display(trainNB_X.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3vycYBnHFbQw"
   },
   "source": [
    "* There are so many algorithms that can be used to do classification, so let's just start with a simple one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CFlNAZz-mtvG"
   },
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "modelNB = GaussianNB()\n",
    "modelNB.fit(trainNB_X, trainNB_Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LqZlk_y7FbQw"
   },
   "source": [
    "* The following is a helper function to display the results in a prettier format than the default.\n",
    "* It display what's known as a <font color='blue'>confusion matrix</font> which shows how many true and false negatives and positives we have.\n",
    "* By looking at these numbers and comparing different models, we can determine which model does the best job for our use case.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LBjpl7bmmtvG"
   },
   "outputs": [],
   "source": [
    "def evaluate_predictions(test, pred, show_percent = True, show_details = False):\n",
    "    from sklearn.metrics import confusion_matrix\n",
    "    length = len(test)\n",
    "    cm = confusion_matrix(test, pred)\n",
    "    test_vc = pd.Series(test).value_counts()\n",
    "    pred_vc = pd.Series(pred).value_counts()\n",
    "    if show_details:\n",
    "        print(f'Test length = {length}')\n",
    "        print('\\nTest Values')\n",
    "        print(test_vc)\n",
    "        print('\\nPredicted Values')\n",
    "        print(pred_vc)\n",
    "        print('\\n TP FN\\n FP TN')\n",
    "        print(cm)\n",
    "\n",
    "\n",
    "    print(f'''\n",
    "A |\\t\\tPredicted\n",
    "c |\\tTP/FP\\t|\\tFN/TN\\t|\\tAP/AN\n",
    "t +---------------------------------------------\n",
    "u |\\t{cm[0,0]:>7}\\t|\\t{cm[0, 1]:>7}\\t|\\t{test_vc[0]:>7}\n",
    "a |\\t{cm[1,0]:>7}\\t|\\t{cm[1, 1]:>7}\\t|\\t{test_vc[1]:>7}\n",
    "l |\\t{pred_vc[0]:>7}\\t|\\t{pred_vc[1]:>7}\\t|\\t{length:>7}\n",
    "''')\n",
    "\n",
    "    if show_percent:\n",
    "        import numpy as np\n",
    "        print('\\n PC FP\\n FN PW')\n",
    "        print(np.ndarray(shape = (2,2), buffer = np.array([round(100 *(cm[0][0] + cm[1][1])/length, 1), \\\n",
    "           round(100 * cm[0][1]/length, 1), round(100 * cm[1][0]/length, 1), round(100 * (cm[1][0] + cm[0][1])/length, 1)])))\n",
    "\n",
    "\n",
    "\n",
    "predNB_Y = modelNB.predict(testNB_X)\n",
    "evaluate_predictions(testNB_Y, predNB_Y, show_details = False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lx8eJeS9FbQw"
   },
   "source": [
    "* Once we've trained a model and decide it's useful, we usually save it to a standard format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cZ-3q6svmtvG"
   },
   "outputs": [],
   "source": [
    "from joblib import dump, load\n",
    "dump(modelNB, 'modelNB.joblib')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cpWvFmjHFbQw"
   },
   "source": [
    "* Once we have a saved model, we can use it in production to make batch or individual predictions by loading that model into something like a web server and exposing it as a web service.\n",
    "  * Remember <font color='blue'>Flask</font>?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "beHkEpLWmtvG"
   },
   "outputs": [],
   "source": [
    "modelNB2 = load('modelNB.joblib')\n",
    "predNB_Y = modelNB2.predict(testNB_X)\n",
    "\n",
    "evaluate_predictions(testNB_Y, predNB_Y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0Qu5nkY2FbQx"
   },
   "source": [
    "* Some algorithms require a special preprocessing of categorical columns into something called <font color='blue'>One Hot Encoding</font> or <font color='blue'>Dummy encoding</font>.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bawE0HyMmtvG"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import preprocessing as pp\n",
    "\n",
    "def dummy_code(data, columns, drop_first = True):\n",
    "    for c in columns:\n",
    "        dummies = pd.get_dummies(data[c], prefix = c, drop_first = drop_first)\n",
    "        i = list(data.columns).index(c)\n",
    "        data = pd.concat([data.iloc[:,:i], dummies, data.iloc[:,i+1:]], axis = 1)\n",
    "    return data\n",
    "\n",
    "dfLR = dummy_code(df, ['type'], drop_first = False)\n",
    "trainLR_X, testLR_X, trainLR_Y, testLR_Y = train_test_split(dfLR.iloc[:,dfLR.columns != 'isFraud'], dfLR.isFraud, train_size = train_size, test_size = test_size, random_state = 1)\n",
    "\n",
    "print(testLR_X.columns)\n",
    "display(testLR_X.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NZlf2zQuFbQx"
   },
   "source": [
    "* From here there are many more algorithms that can be used and some have many hyperparameters we can experiment with.\n",
    "* Our job is to try as many combinations as we can to find the best model and productionize that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tn1rg7y9mtvG"
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "modelLR = LogisticRegression(multi_class='auto', solver='lbfgs', max_iter=1000)\n",
    "modelLR.fit(trainLR_X, trainLR_Y)\n",
    "print(modelLR.coef_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PYeIYSuemtvG"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "predLR_Y = modelLR.predict(testLR_X)\n",
    "\n",
    "score = modelLR.score(testLR_X, testLR_Y)\n",
    "mse = np.mean((predLR_Y - testLR_Y)**2)\n",
    "print(score, mse, '\\n')\n",
    "\n",
    "evaluate_predictions(testLR_Y, predLR_Y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Dq4bzMmEFbQx"
   },
   "source": [
    "### Something to explore on your own.\n",
    "* Sometimes you can influence the false positive/false negative values by changing threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SGenWBAXmtvG"
   },
   "outputs": [],
   "source": [
    "predLR_Y = modelLR.predict_proba(testLR_X)\n",
    "print(predLR_Y[:10])\n",
    "print('Score', modelLR.score(testLR_X, testLR_Y))\n",
    "\n",
    "for threshold in range(10, 91, 10):\n",
    "    predLR_Y1 = np.where(predLR_Y[:,0] >= threshold/100, 0, 1)\n",
    "    mse = np.mean((predLR_Y1 - testLR_Y)**2)\n",
    "    print ('\\nTHRESHOLD', threshold, 'MSE', mse)\n",
    "\n",
    "    evaluate_predictions(testLR_Y, predLR_Y1, show_percent = False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5Hb-t44jFbQx"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "include_colab_link": true,
   "name": "06-01-ClusterAnalysis.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
